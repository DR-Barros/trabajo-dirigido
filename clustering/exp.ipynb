{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cdsgd import DSClustering\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.datasets import load_iris,load_wine\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dunn_index(X, labels):\n",
    "    unique_labels = np.unique(labels)\n",
    "    k = len(unique_labels)\n",
    "    \n",
    "    # Calcular el diámetro de cada clúster\n",
    "    diameters = []\n",
    "    for label in unique_labels:\n",
    "        cluster_points = X[labels == label]\n",
    "        if len(cluster_points) > 1:\n",
    "            diameters.append(np.max(cdist(cluster_points, cluster_points, metric='euclidean')))\n",
    "        else:\n",
    "            diameters.append(0)\n",
    "    \n",
    "    max_diameter = np.max(diameters)\n",
    "    \n",
    "    # Calcular la distancia mínima entre clusters\n",
    "    min_distances = []\n",
    "    for i in range(k):\n",
    "        for j in range(i + 1, k):\n",
    "            cluster_i_points = X[labels == unique_labels[i]]\n",
    "            cluster_j_points = X[labels == unique_labels[j]]\n",
    "            min_distance = np.min(cdist(cluster_i_points, cluster_j_points, metric='euclidean'))\n",
    "            min_distances.append(min_distance)\n",
    "    \n",
    "    min_intercluster_distance = np.min(min_distances)\n",
    "    \n",
    "    # Índice de Dunn\n",
    "    dunn_index_value = min_intercluster_distance / max_diameter\n",
    "    \n",
    "    return dunn_index_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atom = pd.read_csv('data/Atom_Data.csv')\n",
    "atom_labels = pd.read_csv('data/Atom_Labels.csv')\n",
    "chainlink = pd.read_csv('data/Chainlink_Data.csv')\n",
    "chainlink_labels = pd.read_csv('data/Chainlink_Labels.csv')\n",
    "engytime = pd.read_csv('data/EngyTime_Data.csv')\n",
    "engytime_labels = pd.read_csv('data/EngyTime_Labels.csv')\n",
    "hepta = pd.read_csv('data/Hepta_Data.csv')\n",
    "hepta_labels = pd.read_csv('data/Hepta_Labels.csv')\n",
    "tetra = pd.read_csv('data/Tetra_Data.csv')\n",
    "tetra_labels = pd.read_csv('data/Tetra_Labels.csv')\n",
    "target = pd.read_csv('data/Target_Data.csv')\n",
    "target_labels = pd.read_csv('data/Target_Labels.csv')\n",
    "two_diamonds = pd.read_csv('data/TwoDiamonds_Data.csv')\n",
    "two_diamonds_labels = pd.read_csv('data/TwoDiamonds_Labels.csv')\n",
    "wing_nut = pd.read_csv('data/WingNut_Data.csv')\n",
    "wing_nut_labels = pd.read_csv('data/WingNut_Labels.csv')\n",
    "# Cargamos los datasets clasico de sklearn\n",
    "iris = load_iris()\n",
    "iris_data = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "iris_labels = pd.DataFrame(iris.target, columns=['target'])\n",
    "wine = load_wine()\n",
    "wine_data = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "wine_labels = pd.DataFrame(wine.target, columns=['target'])\n",
    "# Cargamos los datasets de prueba de la tesis\n",
    "uniform = pd.read_csv('data/uniform_df.csv')\n",
    "uniform_data = uniform.drop(columns=['labels'])\n",
    "uniform_labels = pd.DataFrame(uniform['labels'])\n",
    "rectangle = pd.read_csv('data/rectangle_df.csv')\n",
    "rectangle_data = rectangle.drop(columns=['labels'])\n",
    "rectangle_labels = pd.DataFrame(rectangle['labels'])\n",
    "gaussian = pd.read_csv('data/gaussian_df.csv')\n",
    "gaussian_data = gaussian.drop(columns=['labels'])\n",
    "gaussian_labels = pd.DataFrame(gaussian['labels'])\n",
    "gaussian_mix = pd.read_csv('data/gaussian_mix_df.csv')\n",
    "gaussian_mix_data = gaussian_mix.drop(columns=['labels'])\n",
    "gaussian_mix_labels = pd.DataFrame(gaussian_mix['labels'])\n",
    "datasets = [\n",
    "    {\n",
    "        'name': 'Atom',\n",
    "        'data': atom,\n",
    "        'labels': atom_labels,\n",
    "        'n_clusters': atom_labels.nunique().values[0]\n",
    "    },{\n",
    "        'name': 'Chainlink',\n",
    "        'data': chainlink,\n",
    "        'labels': chainlink_labels,\n",
    "        'n_clusters': chainlink_labels.nunique().values[0]\n",
    "    },{\n",
    "        'name': 'EngyTime',\n",
    "        'data': engytime,\n",
    "        'labels': engytime_labels,\n",
    "        'n_clusters': engytime_labels.nunique().values[0]\n",
    "    },{\n",
    "        'name': 'Hepta',\n",
    "        'data': hepta,\n",
    "        'labels': hepta_labels,\n",
    "        'n_clusters': hepta_labels.nunique().values[0]\n",
    "    },{\n",
    "        'name': 'Tetra',\n",
    "        'data': tetra,\n",
    "        'labels': tetra_labels,\n",
    "        'n_clusters': tetra_labels.nunique().values[0]\n",
    "    },{\n",
    "        'name': 'Target',\n",
    "        'data': target,\n",
    "        'labels': target_labels,\n",
    "        'n_clusters': target_labels.nunique().values[0]\n",
    "    },{\n",
    "        'name': 'TwoDiamonds',\n",
    "        'data': two_diamonds,\n",
    "        'labels': two_diamonds_labels,\n",
    "        'n_clusters': two_diamonds_labels.nunique().values[0]\n",
    "    },{\n",
    "        'name': 'WingNut',\n",
    "        'data': wing_nut,\n",
    "        'labels': wing_nut_labels,\n",
    "        'n_clusters': wing_nut_labels.nunique().values[0]\n",
    "    },{\n",
    "        'name': 'Iris',\n",
    "        'data': iris_data,\n",
    "        'labels': iris_labels,\n",
    "        'n_clusters': 3\n",
    "    },{\n",
    "        'name': 'Wine',\n",
    "        'data': wine_data,\n",
    "        'labels': wine_labels,\n",
    "        'n_clusters': 3\n",
    "    },{\n",
    "        'name': 'Uniform',\n",
    "        'data': uniform_data,\n",
    "        'labels': uniform_labels,\n",
    "        'n_clusters': uniform_labels.nunique().values[0]\n",
    "    },{\n",
    "        'name': 'Rectangle',\n",
    "        'data': rectangle_data,\n",
    "        'labels': rectangle_labels,\n",
    "        'n_clusters': rectangle_labels.nunique().values[0]\n",
    "    },{\n",
    "        'name': 'Gaussian',\n",
    "        'data': gaussian_data,\n",
    "        'labels': gaussian_labels,\n",
    "        'n_clusters': gaussian_labels.nunique().values[0]\n",
    "    },{\n",
    "        'name': 'GaussianMix',\n",
    "        'data': gaussian_mix_data,\n",
    "        'labels': gaussian_mix_labels,\n",
    "        'n_clusters': gaussian_mix_labels.nunique().values[0]\n",
    "    }\n",
    "]\n",
    "#Resultados\n",
    "results_df = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iteramos sobre los datasets\n",
    "# testear al menos 10 casoss, evaluar diferencias con Ricardo\n",
    "# buscar definiciones de interpretaabilidad y como lo miden\n",
    "for i in range(20):\n",
    "    for dataset in datasets:\n",
    "        n_clusters = dataset['n_clusters']\n",
    "        data = dataset['data']\n",
    "        labels = dataset['labels'].values.ravel()\n",
    "        #normalizamos los datos\n",
    "        scaler = StandardScaler()\n",
    "        data = scaler.fit_transform(data)\n",
    "        data = pd.DataFrame(data, columns=dataset['data'].columns)\n",
    "\n",
    "        #KMeans\n",
    "        kmeans = KMeans(n_clusters=n_clusters)\n",
    "        kmeans_labels = kmeans.fit_predict(data)\n",
    "        kmeans_silhouette = silhouette_score(data, kmeans_labels)\n",
    "        kmeans_dunn = dunn_index(data, kmeans_labels)\n",
    "        kmeans_rand = adjusted_rand_score(labels, kmeans_labels)\n",
    "        kmeans_pearson = pearsonr(labels, kmeans_labels)\n",
    "        results_df = results_df + [{\n",
    "            'Dataset': dataset['name'],\n",
    "            'Algorithm': 'KMeans',\n",
    "            'Silhouette': kmeans_silhouette,\n",
    "            'Dunn': kmeans_dunn,\n",
    "            'Rand': kmeans_rand,\n",
    "            'Pearson': kmeans_pearson[0]\n",
    "        }]\n",
    "\n",
    "        #DBSCAN\n",
    "        dbscan = DBSCAN(eps=0.5)\n",
    "        dbscan_labels = dbscan.fit_predict(data)\n",
    "        if len(np.unique(dbscan_labels)) > 1:\n",
    "            dbscan_silhouette = silhouette_score(data, dbscan_labels)\n",
    "            dbscan_dunn = dunn_index(data, dbscan_labels)\n",
    "            dbscan_rand = adjusted_rand_score(labels, dbscan_labels)\n",
    "            dbscan_pearson = pearsonr(labels, dbscan_labels)\n",
    "        else:\n",
    "            dbscan_silhouette = 0\n",
    "            dbscan_dunn = 0\n",
    "            dbscan_rand = 0\n",
    "            dbscan_pearson = [0, 0]\n",
    "        results_df = results_df + [{\n",
    "            'Dataset': dataset['name'],\n",
    "            'Algorithm': 'DBSCAN',\n",
    "            'Silhouette': dbscan_silhouette,\n",
    "            'Dunn': dbscan_dunn,\n",
    "            'Rand': dbscan_rand,\n",
    "            'Pearson': dbscan_pearson[0]\n",
    "        }]\n",
    "\n",
    "        #Agglomerative\n",
    "        agglomerative = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "        agglomerative_labels = agglomerative.fit_predict(data)\n",
    "        agglomerative_silhouette = silhouette_score(data, agglomerative_labels)\n",
    "        agglomerative_dunn = dunn_index(data, agglomerative_labels)\n",
    "        agglomerative_rand = adjusted_rand_score(labels, agglomerative_labels)\n",
    "        agglomerative_pearson = pearsonr(labels, agglomerative_labels)\n",
    "        results_df = results_df + [{\n",
    "            'Dataset': dataset['name'],\n",
    "            'Algorithm': 'Agglomerative',\n",
    "            'Silhouette': agglomerative_silhouette,\n",
    "            'Dunn': agglomerative_dunn,\n",
    "            'Rand': agglomerative_rand,\n",
    "            'Pearson': agglomerative_pearson[0]\n",
    "        }]\n",
    "\n",
    "        #CDSDG\n",
    "        cdsgd = DSClustering(data=data.copy())\n",
    "        cdsgd.generate_categorical_rules()\n",
    "        cdsgd_labels = cdsgd.predict()\n",
    "        if len(np.unique(cdsgd_labels)) > 1:\n",
    "            cdsgd_silhouette = silhouette_score(data, cdsgd_labels)\n",
    "            cdsgd_dunn = dunn_index(data, cdsgd_labels)\n",
    "            cdsgd_rand = adjusted_rand_score(labels, cdsgd_labels)\n",
    "            cdsgd_pearson = pearsonr(labels, cdsgd_labels)\n",
    "        else:\n",
    "            cdsgd_silhouette = 0\n",
    "            cdsgd_dunn = 0\n",
    "            cdsgd_rand = 0\n",
    "            cdsgd_pearson = [0, 0]\n",
    "        results_df = results_df + [{\n",
    "            'Dataset': dataset['name'],\n",
    "            'Algorithm': 'CDSDG Clustering',\n",
    "            'Silhouette': cdsgd_silhouette,\n",
    "            'Dunn': cdsgd_dunn,\n",
    "            'Rand': cdsgd_rand,\n",
    "            'Pearson': cdsgd_pearson[0]\n",
    "        }]\n",
    "\n",
    "        #CDSDG mas votados\n",
    "        cdsgd1 = DSClustering(data=data.copy(), most_voted=True)\n",
    "        cdsgd1.generate_categorical_rules()\n",
    "        cdsgd1_labels = cdsgd1.predict()\n",
    "        if len(np.unique(cdsgd1_labels)) > 1:\n",
    "            cdsgd1_silhouette = silhouette_score(data, cdsgd1_labels)\n",
    "            cdsgd1_dunn = dunn_index(data, cdsgd1_labels)\n",
    "            cdsgd1_rand = adjusted_rand_score(labels, cdsgd1_labels)\n",
    "            cdsgd1_pearson = pearsonr(labels, cdsgd1_labels)\n",
    "        else:\n",
    "            cdsgd1_silhouette = 0\n",
    "            cdsgd1_dunn = 0\n",
    "            cdsgd1_rand = 0\n",
    "            cdsgd1_pearson = [0, 0]\n",
    "        results_df = results_df + [{\n",
    "            'Dataset': dataset['name'],\n",
    "            'Algorithm': 'CDSDG Voting',\n",
    "            'Silhouette': cdsgd1_silhouette,\n",
    "            'Dunn': cdsgd1_dunn,\n",
    "            'Rand': cdsgd1_rand,\n",
    "            'Pearson': cdsgd1_pearson[0],\n",
    "        }]\n",
    "\n",
    "        # CDSDG con numero de clusters\n",
    "        cdsgd2 = DSClustering(data=data.copy(), cluster=n_clusters)\n",
    "        cdsgd2.generate_categorical_rules()\n",
    "        cdsgd2_labels = cdsgd2.predict()\n",
    "        if len(np.unique(cdsgd2_labels)) > 1:\n",
    "            cdsgd2_silhouette = silhouette_score(data, cdsgd2_labels)\n",
    "            cdsgd2_dunn = dunn_index(data, cdsgd2_labels)\n",
    "            cdsgd2_rand = adjusted_rand_score(labels, cdsgd2_labels)\n",
    "            cdsgd2_pearson = pearsonr(labels, cdsgd2_labels)\n",
    "        else:\n",
    "            cdsgd2_silhouette = 0\n",
    "            cdsgd2_dunn = 0\n",
    "            cdsgd2_rand = 0\n",
    "            cdsgd2_pearson = [0, 0]\n",
    "        results_df = results_df + [{\n",
    "            'Dataset': dataset['name'],\n",
    "            'Algorithm': 'CDSDG Clustering with n_clusters',\n",
    "            'Silhouette': cdsgd2_silhouette,\n",
    "            'Dunn': cdsgd2_dunn,\n",
    "            'Rand': cdsgd2_rand,\n",
    "            'Pearson': cdsgd2_pearson[0]\n",
    "        }]\n",
    "        print(\"Dataset: \", dataset['name'])\n",
    "        print(\"-----------------------------\")\n",
    "        print(\"KMeans: \", kmeans_silhouette, kmeans_dunn, kmeans_rand, kmeans_pearson)\n",
    "        print(\"DBSCAN: \", dbscan_silhouette, dbscan_dunn, dbscan_rand, dbscan_pearson)\n",
    "        print(\"Agglomerative: \", agglomerative_silhouette, agglomerative_dunn, agglomerative_rand, agglomerative_pearson)\n",
    "        print(\"CDSDG Clustering: \", cdsgd_silhouette, cdsgd_dunn, cdsgd_rand, cdsgd_pearson)\n",
    "        print(\"CDSDG Voting: \", cdsgd1_silhouette, cdsgd1_dunn, cdsgd1_rand, cdsgd1_pearson)\n",
    "        print(\"CDSDG Clustering with n_clusters: \", cdsgd2_silhouette, cdsgd2_dunn, cdsgd2_rand, cdsgd2_pearson)\n",
    "    #save results\n",
    "    results = pd.DataFrame(results_df)\n",
    "    results.to_csv('results/results'+str(i)+'.csv', index=False)\n",
    "    results_df = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluamos los resultados\n",
    "for i in range(20):\n",
    "    results = pd.read_csv('results/results'+str(i)+'.csv')\n",
    "    for dataset in results['Dataset'].unique():\n",
    "        dataset_results = results[results['Dataset'] == dataset]\n",
    "        print(\"Dataset: \", dataset)\n",
    "        print(\"-----------------------------\")\n",
    "        print(dataset_results)\n",
    "        print(\"-----------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
