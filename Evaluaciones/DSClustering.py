from dsgd import DSClassifierMultiQ
import pandas as pd
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
import numpy as np
from scipy.optimize import linear_sum_assignment
from scipy.stats import pearsonr

class DSClustering:
    def __init__(self, X, n_clusters, clusters_algorithms):
        self.X = X
        self.n_clusters = n_clusters
        self.clusters_algorithms = clusters_algorithms
        self.results = {}
        self.labels = None
        self.dsc = None

    def compute_clusters(self):
        """
        Compute clusters using the specified clustering algorithms.
        Stores the labels in the `results` dictionary.
        """
        results = {}
        for model in self.clusters_algorithms:
            # Set n_clusters if applicable
            if hasattr(model, 'n_clusters') and getattr(model, 'n_clusters', None) is None:
                model.n_clusters = self.n_clusters
            
            model_name = model.__class__.__name__

            try:
                model.fit(self.X)
                if hasattr(model, 'labels_'):
                    labels = model.labels_
                else:
                    labels = model.fit_predict(self.X)
                results[model_name] = labels
            except Exception as e:
                raise ValueError(f"Error fitting model {model_name}: {e}")
        
        self.results = results

    def compare_clusters(self):
        """
        Compare the clusters generated by different algorithms.
        Prints contingency tables for all pairs.
        """
        if not self.results:
            raise ValueError("No clusters computed. Please run compute_clusters() first.")

        algorithms = list(self.results.keys())
        for i in range(len(algorithms)):
            for j in range(i + 1, len(algorithms)):
                alg1, alg2 = algorithms[i], algorithms[j]
                labels1, labels2 = self.results[alg1], self.results[alg2]
                contingency_table = pd.crosstab(pd.Series(labels1, name=alg1),
                                                pd.Series(labels2, name=alg2))
                print(f"Contingency Table between '{alg1}' and '{alg2}':")
                print(contingency_table)
                print()

    def normalize_clusters(self):
        """
        Permute cluster labels for each algorithm (except the first) to maximize Pearson correlation
        with the reference algorithm's labels, while preserving the number of unique classes.
        """
        if not self.results:
            raise ValueError("No clusters computed. Please run compute_clusters() first.")

        algorithms = list(self.results.keys())
        ref_labels = self.results[algorithms[0]]

        for alg in algorithms[1:]:
            labels = self.results[alg]
            unique_ref = np.unique(ref_labels)
            unique_labels = np.unique(labels)

            # Build cost matrix (negative absolute Pearson correlation for each label mapping)
            cost_matrix = np.zeros((len(unique_labels), len(unique_ref)))
            for i, l1 in enumerate(unique_labels):
                for j, l2 in enumerate(unique_ref):
                    mask1 = (labels == l1).astype(int)
                    mask2 = (ref_labels == l2).astype(int)
                    # If all zeros or all ones, pearsonr returns nan, so set to 0
                    try:
                        corr, _ = pearsonr(mask1, mask2)
                        if np.isnan(corr):
                            corr = 0
                    except Exception:
                        corr = 0
                    cost_matrix[i, j] = -abs(corr)

            # Hungarian algorithm to maximize total absolute Pearson correlation
            row_ind, col_ind = linear_sum_assignment(cost_matrix)
            label_mapping = {unique_labels[i]: unique_ref[j] for i, j in zip(row_ind, col_ind)}

            # Remap labels, keep unmapped labels as is (if more clusters in one than the other)
            new_labels = np.array([label_mapping.get(l, l) for l in labels])
            self.results[alg] = new_labels
        
        
    def fit_dsc(self, threshold=0.01):
        X_values = self.X.values
        # Concatenate all cluster labels from different algorithms into a single vector
        Y_values = np.concatenate([labels for labels in self.results.values()])
        X_values2 = np.concatenate([X_values] * len(self.results), axis=0)
        """ print("Fitting DSC with the following parameters:")
        print(f"Number of clusters: {self.n_clusters}")
        print(f"Shape of X_values: {X_values2.shape}")
        print(f"Shape of Y_values: {Y_values.shape}") """
        #recorremos X_values2 y Y_values y printeamos
        """ for i in range(len(X_values2)):
            print(f"X_values2[{i}]: {X_values2[i]}, Y_values[{i}]: {Y_values[i]}") """
        #cantidad de elementos en cada cluster
        print("Number of elements in each cluster:")
        for i in range(self.n_clusters):
            count = np.sum(Y_values == i)
            print(f"Cluster {i}: {count} elements")
        DSC = DSClassifierMultiQ(
            self.n_clusters,
            min_iter=20,
            max_iter=2000,
            debug_mode=True,
            lossfn="MSE",
            num_workers=0,
            min_dloss=1e-7,
            lr=0.01,
            precompute_rules=True
        )
        losses, epoch, dt = DSC.fit(
            X_values2,
            Y_values,
            add_single_rules=True,
            single_rules_breaks=3,
            add_mult_rules=False,
            column_names=self.X.columns,
            print_every_epochs=1
        )
        """ for alg, labels in self.results.items():
            # Ensure labels are reshaped correctly
            Y_values = labels
            losses, epoch, dt = DSC.fit(
                X_values,
                Y_values,
                add_single_rules=True,
                single_rules_breaks=3,
                add_mult_rules=False,
                column_names=self.X.columns,
                print_every_epochs=1
            )
            DSC.print_most_important_rules(threshold=0.01)   """
        predictions = DSC.predict(X_values)
        self.labels = predictions
        DSC.print_most_important_rules(threshold=threshold)
        self.dsc = DSC
        return self.labels
